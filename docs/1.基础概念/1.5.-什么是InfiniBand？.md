# 什么是InfiniBand？

> InfiniBand (IB) 是一个计算机网络通信标准，在高性能计算（HPC）领域有广泛的应用，可以提供高吞吐带宽和超低的网络传输时延。IB可以用于计算机内部或外部的数据互联。通过直连或者交换机互联的方式，提供服务器与存储，存储设备之间的高性能网络。IB网络可以通过交换机网络实现横向扩展，适应各种规模的组网需求。





​	InfiniBand是一种强大的新架构，旨在支持互联网的I/O连接基础设施。InfiniBand是所有主要OEM服务器供应商支持的一种手段在服务器中扩展并创建下一代I/O互连标准。首先时间，高容量，工业标准的I/O互连扩展了传统的BUS 功能。InfiniBand在提供“盒内”背板解决方案和外部互连和“带宽开箱即用”，因此它提供了一种以前仅为传统网络互连保留的连接方式。这种I/O和系统的统一区域网络需要一种新的体系结构来支持这两个先前独立的域的需求。这种主要I/O转换的基础是InfiniBand支持互联网需求的能力对于RAS:可靠性、可用性和可服务性。

![基于NVIDIA QM8700/8790交换机与HDR网卡的InfiniBand高性能网络解决方案_数据](https://s2.51cto.com/images/blog/202211/03174141_63638cd5c99b57023.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184/format,webp)

 

​	作为计算机集群互联技术，IB技术相比以太网/Fibre Channel 和已经淘汰的Omni-Path技术有明显的优势，是InfiniBand Trade Association（IBTA）主要推荐的网络通信技术。从2014年开始，大多数的TOP500 超级计算机都采用了InfiniBand网络技术。近些年，AI/大数据相关的应用，也已经大规模的采用IB网络实现高性能的集群部署，其中Top100的超算中心有62%是使用的IB技术。

​	InfiniBand 提供QoS（服务质量）和 RAS（可靠性、可用性、可服务性）。 这些 RAS 功能从一开始就被设计到 InfiniBand 架构中，对于它作为互联网核心的下一代计算服务器和存储系统的通用 I/O 基础设施的能力至关重要

![image-20230505140834240](/Users/ringi/Library/Application Support/typora-user-images/image-20230505140834240.png)

### InfiniBand较Omni-Path的优势

​	尽管NVIDIA已经推出了IB 400G NDR的解决方案，但现阶段仍有部分客户在使用100G解决方案。对于100G的高性能网络，常用的方案有Omni-Path和IB两种，虽然速率相同，性能相似，但网络结构差异巨大。以400节点集群为例，使用IB方案只需要15台NVIDIA Quantum 8000系列交换机+200条200G分支线缆与200条200G直连线缆；而使用Omni-Path则需要24台交换机+876条100G直连线缆（384节点）。IB方案在前期的设备成本以及后期运维成本上都极具优势，且整体功耗远低于Omni-Path，更加环保。



![img](https://s2.51cto.com/images/blog/202211/03174140_63638cd500b1a99971.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp)

### I/O 架构设计 - Fabric（IB） vs. Bus（PCI）

#### Shared Bus Architecture

> 共享总线架构是最常见的今天的 I/O 互连虽然有许多缺点。 集群和网络需要具有高速容错能力的系统无法正确支持的互连采用总线架构。 因此，所有总线架构都需要网络接口模块来启用可扩展的网络拓扑。 为了与系统保持同步，I/O 架构必须提供具有扩展能力的高速连接。在总线架构中，所有通信共享相同的带宽。 添加到总线上的端口越多，每个外围设备可用的带宽就越少。 它们还存在严重的电气、机械和电源问题。 在并行总线上，每个连接都需要许多引脚（64 位 PCI 需要 90 个引脚），这使得电路板的布局非常棘手，并且占用了宝贵的印刷电路板 (PCB) 空间。 在高总线频率下，每个信号的距离仅限于 PCB 板上的短走线。 在具有多个卡槽的基于槽的系统中，端接不受控制，如果设计不当可能会导致问题

![image-20230505141901760](/Users/ringi/Library/Application Support/typora-user-images/image-20230505141901760.png)

#### Switched Fabric Architecture

> 交换结构是一种基于点对点交换的互连，专为容错和可扩展性而设计。 点对点交换结构意味着每条链路只有一个设备连接在链路的每一端。 因此，负载和终端特性得到很好的控制（与总线架构不同），只允许一个设备，最坏的情况与典型情况相同，因此 I/O 性能可以通过结构提高得多。交换结构体系结构提供了可扩展性，这可以通过向结构中添加交换机并通过交换机连接更多端节点来实现。 与共享总线架构不同，系统的总带宽会随着额外交换机添加到网络而增加。 设备之间的多条路径保持高聚合带宽并提供故障安全、冗余连接。

![image-20230505142153484](/Users/ringi/Library/Application Support/typora-user-images/image-20230505142153484.png)

InfiniBand 架构分为多个层，其中每一层彼此独立运行。 如图 5所示，“InfiniBand 层”InfiniBand 分为
以下层：物理层、链路层、网络层、传输层和上层

![image-20230505142935384](/Users/ringi/Library/Application Support/typora-user-images/image-20230505142935384.png)

> InfiniBand 架构为系统通信定义了多种设备：通道适配器、交换机、路由器和子网管理器。 在一个子网内，每个端节点必须至少有一个通道适配器和一个子网管理器来建立和维护链路。 所有通道适配器和交换机都必须包含处理与子网管理器通信所需的子网管理代理 (SMA)

![image-20230505143052980](/Users/ringi/Library/Application Support/typora-user-images/image-20230505143052980.png)